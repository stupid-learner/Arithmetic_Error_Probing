#!/usr/bin/env python3
"""
GSM8K Neural Probe and Error Detector Visualization Script

This script generates visualization plots for probe and error detector performance results
from JSON files generated by the GSM8K testing script.

Usage:
python visualize_testing_pure_on_gsm8k.py --probe_results test_2_shots_probe_on_gsm8k_gemma-2-2b-it.json \
                                  --error_results test_2_shots_error_detector_on_gsm8k_gsm8k_gemma-2-2b-it.json \
                                  --output_folder plots_test_pure_on_gsm8k
"""

import matplotlib.pyplot as plt
import numpy as np
import os
import json
import seaborn as sns
import argparse
from matplotlib.gridspec import GridSpec


# Global font settings - extremely large fonts for LaTeX visibility
plt.rcParams.update({
    'font.size': 40,             # Base font size
    'axes.titlesize': 60,        # Axes title
    'axes.labelsize': 52,        # Axes labels
    'xtick.labelsize': 40,       # X-axis tick labels
    'ytick.labelsize': 40,       # Y-axis tick labels
    'legend.fontsize': 48,       # Legend
    'figure.titlesize': 64       # Figure title
})

def estimate_majority_ratio(error_results):
    
    def _try(detector_dict):
        last = sorted(detector_dict, key=lambda x: x["layer"])[-1]
        a, p, r = last["accuracy"], last["precision"], last["recall"]

        if p in (0, 1) or r in (0, 1):
            return None
        k = r * (2 * p - 1) / p
        if abs(1 - k) < 1e-6:         
            return None
        t = (1 - a) / (1 - k)
        if 0 <= t <= 1:
            return max(t, 1 - t)
        return None

    for det in ["mlp", "circular_jointly", "mlp_seperately",
                "circular_seperately", "logistic_seperately"]:
        if det in error_results and error_results[det]:
            val = _try(error_results[det])
            if val is not None:
                return val

    return None


def load_probe_results(probe_json_path):
    """
    Load probe results from JSON file
    """
    try:
        with open(probe_json_path, 'r') as f:
            data = json.load(f)
            
        # Check if any data was loaded
        if not data:
            print(f"Warning: No data found in {probe_json_path}")
            return {}
            
        print(f"Successfully loaded probe results from {probe_json_path}")
        return data
    except Exception as e:
        print(f"Error loading probe results from {probe_json_path}: {e}")
        return {}


def load_error_detector_results(error_json_path):
    """Load error detector results from JSON file"""
    try:
        with open(error_json_path, 'r') as f:
            data = json.load(f)
            
        # Check if any data was loaded
        if not data:
            print(f"Warning: No data found in {error_json_path}")
            return {}
        
        # Convert tuple format to dict format if needed
        for detector_type in data:
            if data[detector_type] and not isinstance(data[detector_type][0], dict):
                data[detector_type] = [
                    {
                        "layer": point[0],
                        "accuracy": point[1],
                        "precision": point[2],
                        "recall": point[3],
                        "f1": point[4]
                    }
                    for point in data[detector_type]
                ]
            
        print(f"Successfully loaded error detector results from {error_json_path}")
        return data
    except Exception as e:
        print(f"Error loading error detector results from {error_json_path}: {e}")
        return {}


def visualize_probe_accuracy(probe_results, output_dir):
    """
    Visualize probe accuracy across layers for different probe types and targets
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # Set style
    plt.style.use('seaborn-v0_8-whitegrid')
    sns.set_palette("deep")
    
    # Standard figure size matching the three-subplot comparison subplots
    standard_figsize = (12, 10)
    
    # Define markers and linestyles for consistency
    markers = ['o', 's', 'D', '^']
    linestyles = ['-', '--', '-.', ':']
    
    # Define friendly names for each probe type
    probe_names = {
        "linear": "Linear",
        "mlp": "MLP",
        "circular": "Circular",
        "logistic": "Logistic"
    }
    
    # Process probe results by target
    for target_name, target_friendly_name in {"gt_probe": "Ground Truth", "output_probe": "Model Output"}.items():
        # Create figure with standard size
        fig, ax = plt.subplots(figsize=standard_figsize)
        
        # Check if we have any data for this target
        has_data = False
        
        # Plot each probe type
        for i, probe_type in enumerate(["circular", "linear", "mlp", "logistic"]):
            if probe_type in probe_results and target_name in probe_results[probe_type]:
                data_points = probe_results[probe_type][target_name]
                if data_points:  # Only plot if we have data
                    has_data = True
                    # Sort data points by layer index
                    data_points.sort(key=lambda x: x[0])
                    
                    # Extract layer indices and accuracies
                    layers = [point[0] for point in data_points]
                    accuracies = [point[1] for point in data_points]
                    
                    ax.plot(layers, accuracies, label=probe_names[probe_type], 
                          marker=markers[i % len(markers)],
                          markersize=12,  # Reduced marker size
                          linestyle=linestyles[i % len(linestyles)], 
                          linewidth=3.0)  # Reduced line width
        
        if has_data:
            # Consistent title and labels with reduced size
            ax.set_title(f"Probe Accuracy on {target_friendly_name}", fontsize=36, pad=20)
            ax.set_xlabel("Layer Index", fontsize=32)
            ax.set_ylabel("Accuracy", fontsize=32)
            
            # Set y-axis limits
            ax.set_ylim(0, 1.0)
            ax.set_yticks(np.arange(0, 1.1, 0.1))
            
            # Consistent tick label size (reduced)
            ax.tick_params(axis='both', which='major', labelsize=28, length=8, width=2)
            
            # Consistent legend (reduced size)
            ax.legend(fontsize=30, handlelength=3)
            ax.grid(True, linewidth=1.5)
            
            # Save figure
            plt.tight_layout()
            output_file = os.path.join(output_dir, f"probe_accuracy_{target_name}(pure_on_gsm8k).pdf")
            plt.savefig(output_file, bbox_inches='tight')
            print(f"Saved plot to {output_file}")
            
            plt.close(fig)
        else:
            print(f"No data available for target {target_name}, skipping visualization")
            plt.close(fig)


def visualize_error_detector_metrics(error_results, output_dir):
    """
    Visualize error detector metrics (accuracy, precision, recall, f1) across layers
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # Set style
    plt.style.use('seaborn-v0_8-whitegrid')
    sns.set_palette("deep")
    
    # Standard figure size matching the three-subplot comparison subplots
    standard_figsize = (12, 10)
    
    # Define markers and linestyles for consistency
    markers = ['o', 's', 'D', '^', 'X']
    linestyles = ['-', '--', '-.', ':', '-']
    
    # Define friendly names for each detector type
    detector_names = {
        "logistic_seperately": "Logistic (Separate)",
        "mlp": "MLP (Joint)",
        "mlp_seperately": "MLP (Separate)",
        "circular_seperately": "Circular (Separate)",
        "circular_jointly": "Circular (Joint)"
    }
    
    # Different color palette for error detectors - same as in three-subplot comparison
    error_detector_colors = sns.color_palette("Set2", 5)
    
    # Create plots for each metric
    metrics = [
        ("accuracy", "Error Detector Accuracy"), 
        ("precision", "Error Detector Precision"),
        ("recall", "Error Detector Recall"),
        ("f1", "Error Detector F1-Score")
    ]
    
    for metric_name, title_text in metrics:
        # Create figure with standard size
        fig, ax = plt.subplots(figsize=standard_figsize)
        
        # Check if we have any data for this metric
        has_data = False
        
        # Plot each detector type
        for i, detector_type in enumerate(detector_names.keys()):
            if detector_type in error_results and error_results[detector_type]:
                has_data = True
                # Process the JSON format data
                data_points = error_results[detector_type]
                
                # Sort data points by layer index
                # For the new format, each data point is a dictionary
                if isinstance(data_points[0], dict):
                    # Sort by layer
                    data_points.sort(key=lambda x: x["layer"])
                    
                    # Extract layers and metric values
                    layers = [point["layer"] for point in data_points]
                    values = [point[metric_name] for point in data_points]
                    
                    ax.plot(layers, values, label=detector_names[detector_type], 
                           marker=markers[i % len(markers)],
                           markersize=12,  # Reduced marker size
                           linestyle=linestyles[i % len(linestyles)], 
                           linewidth=3.0,  # Reduced line width
                           color=error_detector_colors[i])  # Use consistent colors
        
        if has_data:
            # Add a dashed line at y=0.5 for accuracy metric (without text)
            if metric_name == "accuracy":
                ax.axhline(y=0.5, color='black', linestyle='--', linewidth=2.0, alpha=0.7)
            
            # Consistent title and labels with reduced size
            ax.set_title(f"{title_text}", fontsize=36, pad=20)
            ax.set_xlabel("Layer Index", fontsize=32)
            ax.set_ylabel(title_text.split(" ")[-1], fontsize=32)  
            
            # Set y-axis limits
            ax.set_ylim(0, 1.0)
            ax.set_yticks(np.arange(0, 1.1, 0.1))
            
            # Consistent tick labels (reduced size)
            ax.tick_params(axis='both', which='major', labelsize=28, length=8, width=2)
            
            # Consistent legend (reduced size)
            ax.legend(fontsize=30, handlelength=3)
            ax.grid(True, linewidth=1.5)
            
            # Save figure
            plt.tight_layout()
            output_file = os.path.join(output_dir, f"error_detector_{metric_name}(pure_on_gsm8k).pdf")
            plt.savefig(output_file, bbox_inches='tight')
            print(f"Saved plot to {output_file}")
        else:
            print(f"No data available for metric {metric_name}, skipping visualization")
            
        plt.close(fig)


def print_last_layer_metrics(error_results):
    """
    Print accuracy, precision, and recall for each error detector at the last layer
    """
    print("\n=== Error Detector Performance at Last Layer (Digit 3) ===")
    print("{:<20} {:<10} {:<10} {:<10} {:<10}".format(
        "Detector Type", "Accuracy", "Precision", "Recall", "F1-Score"))
    print("-" * 65)
    
    # Define friendly names for each detector type
    detector_names = {
        "logistic_seperately": "Logistic (Separate)",
        "mlp": "MLP (Joint)",
        "mlp_seperately": "MLP (Separate)",
        "circular_seperately": "Circular (Separate)",
        "circular_jointly": "Circular (Joint)"
    }
    
    for detector_type, friendly_name in detector_names.items():
        if detector_type in error_results and error_results[detector_type]:
            # Sort by layer to find the last one
            if isinstance(error_results[detector_type][0], dict):
                # Sort by layer
                sorted_data = sorted(error_results[detector_type], key=lambda x: x["layer"])
                last_layer_data = sorted_data[-1]
                
                # Extract metrics
                accuracy = last_layer_data["accuracy"]
                precision = last_layer_data["precision"]
                recall = last_layer_data["recall"]
                f1 = last_layer_data["f1"]
                
                print("{:<20} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}".format(
                    friendly_name, accuracy, precision, recall, f1))
            else:
                print("{:<20} {:<10} {:<10} {:<10} {:<10}".format(
                    friendly_name, "N/A", "N/A", "N/A", "N/A"))
        else:
            print("{:<20} {:<10} {:<10} {:<10} {:<10}".format(
                friendly_name, "N/A", "N/A", "N/A", "N/A"))
    
    print("=" * 65)


def create_three_subplot_comparison(probe_results, error_results, output_dir):
    """
    Create a horizontal arrangement of three subplots showing:
    1. Probe Accuracy on Model Prediction
    2. Probe Accuracy on Ground-Truth Result
    3. Error Detector Accuracy with a dashed line at y=0.5
    
    Parameters:
    -----------
    probe_results : dict
        Dictionary of probe results loaded from load_probe_results()
    error_results : dict
        Dictionary of error detector results loaded from load_error_detector_results()
    output_dir : str
        Directory to save the output figure
    """
    # Set style
    plt.style.use('seaborn-v0_8-whitegrid')
    
    # Create a figure with 3 subplots arranged horizontally
    fig = plt.figure(figsize=(36, 10))
    gs = GridSpec(1, 3, figure=fig, width_ratios=[1, 1, 1])
    ax1 = fig.add_subplot(gs[0, 0])
    ax2 = fig.add_subplot(gs[0, 1])
    ax3 = fig.add_subplot(gs[0, 2])
    
    # Define probe types and their properties
    probe_types = ["circular", "linear", "mlp", "logistic"]
    probe_names = {
        "linear": "Linear",
        "mlp": "MLP",
        "circular": "Circular",
        "logistic": "Logistic"
    }
    
    # Define markers and linestyles for consistency
    markers = ['o', 's', 'D', '^']
    linestyles = ['-', '--', '-.', ':']
    
    # Color palette for probes - using a consistent set of colors
    probe_colors = sns.color_palette("deep", len(probe_types))
    
    # Different color palette for error detectors
    error_detector_colors = sns.color_palette("Set2", 5)
    
    # Plot Model Prediction probes (left subplot)
    for i, probe_type in enumerate(probe_types):
        if probe_type in probe_results and "output_probe" in probe_results[probe_type]:
            data_points = probe_results[probe_type]["output_probe"]
            if data_points:  # Only plot if we have data
                # Sort data points by layer index
                data_points.sort(key=lambda x: x[0])
                
                # Extract layer indices and accuracies
                layers = [point[0] for point in data_points]
                accuracies = [point[1] for point in data_points]
                
                ax1.plot(layers, accuracies, 
                       label=probe_names[probe_type],
                       marker=markers[i % len(markers)], 
                       markersize=14,
                       linestyle=linestyles[i % len(linestyles)], 
                       linewidth=4.0,
                       color=probe_colors[i])
                
    # Plot Ground Truth probes (middle subplot)
    for i, probe_type in enumerate(probe_types):
        if probe_type in probe_results and "gt_probe" in probe_results[probe_type]:
            data_points = probe_results[probe_type]["gt_probe"]
            if data_points:  # Only plot if we have data
                # Sort data points by layer index
                data_points.sort(key=lambda x: x[0])
                
                # Extract layer indices and accuracies
                layers = [point[0] for point in data_points]
                accuracies = [point[1] for point in data_points]
                
                ax2.plot(layers, accuracies, 
                       label=probe_names[probe_type],
                       marker=markers[i % len(markers)], 
                       markersize=14,
                       linestyle=linestyles[i % len(linestyles)], 
                       linewidth=4.0,
                       color=probe_colors[i])
    
    # Plot Error Detector accuracy (Right subplot)
    detector_names = {
        "logistic_seperately": "Logistic (Separate)",
        "mlp": "MLP (Joint)",
        "mlp_seperately": "MLP (Separate)",
        "circular_seperately": "Circular (Separate)",
        "circular_jointly": "Circular (Joint)"
    }
    
    for i, detector_type in enumerate(detector_names.keys()):
        if detector_type in error_results and error_results[detector_type]:
            # Process the JSON format data
            data_points = error_results[detector_type]
            
            # Sort data points by layer index
            # For the new format, each data point is a dictionary
            if isinstance(data_points[0], dict):
                # Sort by layer
                data_points.sort(key=lambda x: x["layer"])
                
                # Extract layers and accuracy values
                layers = [point["layer"] for point in data_points]
                accuracy_values = [point["accuracy"] for point in data_points]
                
                ax3.plot(layers, accuracy_values, 
                       label=detector_names[detector_type],
                       marker=markers[i % len(markers)], 
                       markersize=14,
                       linestyle=linestyles[i % len(linestyles)], 
                       linewidth=4.0,
                       color=error_detector_colors[i])
    
    # Add a dashed line at y=0.5 for error detector plot
    ax3.axhline(y=0.5, color='black', linestyle='--', linewidth=3.0, alpha=0.7)
    
    # Set titles and labels with original font sizes
    ax1.set_title("Probe Accuracy on Model Prediction", fontsize=44, pad=20)   
    ax2.set_title("Probe Accuracy on Ground-Truth", fontsize=44, pad=20)   
    ax3.set_title("Error Detector Accuracy", fontsize=44, pad=20)   
    
    for ax in [ax1, ax2, ax3]:
        ax.set_xlabel("Layer Index", fontsize=40)
        ax.set_ylabel("Accuracy", fontsize=40)
        
        # Set y-axis limits
        ax.set_ylim(0, 1.0)
        ax.set_yticks(np.arange(0, 1.1, 0.1))
        
        # Consistent tick label size
        ax.tick_params(axis='both', which='major', labelsize=32, length=8, width=2)
        
        # Add grid
        ax.grid(True, linewidth=1.5)

    # Add subplot labels (a, b, c)
    label_fontsize = 36
    label_fontweight = 'bold'
    label_color = 'black'
    
    ax1.text(0.5, -0.2, '(a)', transform=ax1.transAxes, fontsize=label_fontsize, 
            fontweight=label_fontweight, color=label_color, va='top', ha='center')
    ax2.text(0.5, -0.2, '(b)', transform=ax2.transAxes, fontsize=label_fontsize, 
            fontweight=label_fontweight, color=label_color, va='top', ha='center')
    ax3.text(0.5, -0.2, '(c)', transform=ax3.transAxes, fontsize=label_fontsize, 
            fontweight=label_fontweight, color=label_color, va='top', ha='center')
    
    # Get handles and labels for legends
    probe_handles, probe_labels = ax1.get_legend_handles_labels()
    detector_handles, detector_labels = ax3.get_legend_handles_labels()
        
    # Create shared legends
    plt.subplots_adjust(bottom=0.5)  
    
    probe_legend = fig.legend(probe_handles, probe_labels, 
                           loc='lower center', bbox_to_anchor=(0.3, -0.3),
                           fontsize=36, ncol=2, title="Probes", title_fontsize=40)
    
    detector_legend = fig.legend(detector_handles, detector_labels, 
                              loc='lower center', bbox_to_anchor=(0.7, -0.3),
                              fontsize=36, ncol=2, title="Error Detectors", title_fontsize=40)
    
    fig.add_artist(probe_legend)
    fig.add_artist(detector_legend)
    
    # Adjust layout
    plt.subplots_adjust(top=0.85, bottom=0.15, left=0.1, right=0.95, wspace=0.25)

    for ax in [ax1, ax2, ax3]:
        pos = ax.get_position()
        new_height = pos.height * 0.95  
        new_bottom = pos.y0 + pos.height - new_height  
        ax.set_position([pos.x0, new_bottom, pos.width, new_height])
    
    # Save figure
    output_file = os.path.join(output_dir, "three_subplot_comparison(pure_on_gsm8k).pdf")
    plt.savefig(output_file, bbox_inches='tight')
    
    output_file = os.path.join(output_dir, "three_subplot_comparison(pure_on_gsm8k).png")
    plt.savefig(output_file, bbox_inches='tight')
    print(f"Saved three-subplot comparison to {output_file}")
    
    plt.close(fig)
    
    return output_file


def main():
    """Main function to run visualizations"""
    parser = argparse.ArgumentParser(description="Visualize GSM8K probe and error detector performance results")
    parser.add_argument("--probe_results", required=True, help="Path to probe results JSON file")
    parser.add_argument("--error_results", required=True, help="Path to error detector results JSON file")
    parser.add_argument("--output_folder", default="gsm8k_plots", help="Directory to save plots")
    
    args = parser.parse_args()
    
    print(f"Loading probe results from {args.probe_results}...")
    probe_results = load_probe_results(args.probe_results)
    
    print(f"Loading error detector results from {args.error_results}...")
    error_results = load_error_detector_results(args.error_results)
    
    print("\nCreating visualizations...")
    
    # Create probe accuracy visualization
    visualize_probe_accuracy(probe_results, args.output_folder)
    
    # Create error detector metric visualizations
    visualize_error_detector_metrics(error_results, args.output_folder)
    
    # Print last layer metrics for each error detector
    print_last_layer_metrics(error_results)
    
    # Create the new three-subplot comparison visualization
    create_three_subplot_comparison(probe_results, error_results, args.output_folder)
    
    print(f"\nAll visualizations saved to {args.output_folder}")


if __name__ == "__main__":
    main()